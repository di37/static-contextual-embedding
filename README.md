# Self-Attention in NLP: From Static to Contextual Embeddings

## Introduction

This repository contains code snippets demonstrating the transition from static word embeddings to contextual embeddings using self-attention mechanisms. The code complements the concepts discussed in the blog post and allows readers to interact with the implementations for a deeper understanding.

## Installation

Make sure to have Anaconda or Miniconda installed in the system.

Clone the repository:
```bash
git clone https://github.com/di37/static-contextual-embeddings
cd static-contextual-embeddings
```

Create a conda environment:
```bash
conda create -n static_contextual_embeddings python=3.12
conda activate static_contextual_embeddings
```

Install the dependencies:
```bash
pip install -r requirements.txt
```

## References

- [NumPy Documentation](https://numpy.org/)
- [Scikit-learn Feature Extraction](https://scikit-learn.org/1.5/modules/feature_extraction.html)
- [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
